{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3daMGk7yHoW"
      },
      "source": [
        "# Adversarial Robustness Testing with PyTorch\n",
        "\n",
        "## This notebook demonstrates:\n",
        "1. Training a simple image classification model\n",
        "2. Attacking the model using adversarial examples\n",
        "3. Measuring robustness degradation\n",
        "4. Applying a basic defense\n",
        "5. Interpreting results for AI security & governance\n",
        "\n",
        "## Why this matters:\n",
        "- Adversarial attacks are a real AI security risk\n",
        "- Regulators and auditors expect robustness evidence\n",
        "- This notebook produces measurable, auditable results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXHi8FNa0q7v"
      },
      "source": [
        "## Why This Notebook Matters\n",
        "\n",
        "### Security:\n",
        "- Demonstrates real adversarial vulnerability\n",
        "- Tests defensive effectiveness\n",
        "\n",
        "### Governance:\n",
        "- Produces measurable risk metrics\n",
        "- Supports AI risk assessments\n",
        "- Aligns with OWASP AI, NIT AI RMF and ISO/IEC 42001 expectations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install the appropriate packages"
      ],
      "metadata": {
        "id": "mfsOv4u6kVj7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSG69vBayL4T",
        "outputId": "a1876192-b75f-4224-8e3a-95db49c758ba",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: adversarial-robustness-toolbox in /usr/local/lib/python3.12/dist-packages (1.20.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cpu)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.12/dist-packages (from adversarial-robustness-toolbox) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from adversarial-robustness-toolbox) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.12/dist-packages (from adversarial-robustness-toolbox) (1.6.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from adversarial-robustness-toolbox) (1.17.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from adversarial-robustness-toolbox) (75.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from adversarial-robustness-toolbox) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.22.2->adversarial-robustness-toolbox) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.22.2->adversarial-robustness-toolbox) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install adversarial-robustness-toolbox torch torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEKbaIAjyQw7"
      },
      "source": [
        "## Imports\n",
        "\n",
        "### We use:\n",
        "- PyTorch for model training\n",
        "- NumPy for numerical operations\n",
        "- ART for attacks and defenses\n",
        "\n",
        "ART works on NumPy arrays, so we will convert tensors where needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Kezjp1x8tCbp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from art.estimators.classification import PyTorchClassifier, SklearnClassifier\n",
        "from art.attacks.evasion import FastGradientMethod, ProjectedGradientDescent\n",
        "from art.defences.preprocessor import FeatureSqueezing\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyoZ1ox2yXmg"
      },
      "source": [
        "## Loading the MNIST Dataset\n",
        "\n",
        "MNIST contains handwritten digits (0‚Äì9).\n",
        "\n",
        "Key points:\n",
        "- Images are grayscale (1 channel)\n",
        "- Values are normalized to [0, 1]\n",
        "- This normalization must match ART's `clip_values`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ffggu1nryYI0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7898b6b-eaf9-47ff-f004-7e3b828ea8bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9.91M/9.91M [00:00<00:00, 19.8MB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28.9k/28.9k [00:00<00:00, 542kB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.65M/1.65M [00:00<00:00, 4.28MB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.54k/4.54k [00:00<00:00, 9.14MB/s]\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "train_dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "# Convert test set to NumPy (ART requires NumPy)\n",
        "x_test = np.concatenate([x.numpy() for x, _ in test_loader], axis=0)\n",
        "y_test = np.concatenate([y.numpy() for _, y in test_loader], axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLvgwrAvzEkP"
      },
      "source": [
        "## Model Architecture\n",
        "\n",
        "### We use a small CNN:\n",
        "- Convolution layer to extract features\n",
        "- Fully connected layers for classification\n",
        "\n",
        "### This model is intentionally simple so that:\n",
        "- Attacks are clearly visible\n",
        "- Robustness degradation is easy to interpret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dTxyZwtty-dl"
      },
      "outputs": [],
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(26 * 26 * 32, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "model = SimpleCNN()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjzyrTXMzTDB"
      },
      "source": [
        "## Training the Model\n",
        "\n",
        "### We train for only a few epochs:\n",
        "- High accuracy is not the goal\n",
        "- Robustness testing is the goal\n",
        "\n",
        "### Security insight:\n",
        "A highly accurate model can still be extremely fragile."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xL3_-zvqzZBu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "304ed5d3-39ad-4302-99c2-9e0960a09b5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training complete\n"
          ]
        }
      ],
      "source": [
        "model.train()\n",
        "for epoch in range(3):\n",
        "    for x, y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(model(x), y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "print(\"Training complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0gWzaIMzed4"
      },
      "source": [
        "## Wrapping the Model with ART\n",
        "\n",
        "### This step allows ART to:\n",
        "- Compute gradients\n",
        "- Generate adversarial examples\n",
        "- Apply defenses\n",
        "\n",
        "The `clip_values` parameter defines valid input boundaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UvJRoD53ze9n"
      },
      "outputs": [],
      "source": [
        "classifier = PyTorchClassifier(\n",
        "    model=model,\n",
        "    loss=criterion,\n",
        "    optimizer=optimizer,\n",
        "    input_shape=(1, 28, 28),\n",
        "    nb_classes=10,\n",
        "    clip_values=(0.0, 1.0)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_BVzsISjwjN"
      },
      "source": [
        "## Utility: ensure_float32\n",
        "This helper function enforces a **dtype contract** between NumPy and PyTorch.\n",
        "\n",
        "### Why it exists:\n",
        "- ART attacks and preprocessors often return `float64`\n",
        "- PyTorch models are typically trained in `float32`\n",
        "- Mixing these causes runtime failures\n",
        "\n",
        "`ensure_float32` guarantees that all data entering the model is compatible and stable, which is critical in agent-based pipelines.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fYyY9S5-CHhX"
      },
      "outputs": [],
      "source": [
        "def ensure_float32(x):\n",
        "    if x.dtype != np.float32:\n",
        "        return x.astype(np.float32)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGyGtMSbzuLx"
      },
      "source": [
        "## Accuracy Helper\n",
        "\n",
        "We use the same function to measure:\n",
        "- Clean accuracy\n",
        "- Adversarial accuracy\n",
        "- Defended accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LGV-HtkFzuzM"
      },
      "outputs": [],
      "source": [
        "def accuracy(x, y):\n",
        "    x = ensure_float32(x)  # enforce contract\n",
        "    preds = classifier.predict(x)\n",
        "    return np.mean(np.argmax(preds, axis=1) == y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "X7Ls6xS20Dvy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3a574dc-4e16-4f6c-dbbf-631d892a968e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clean accuracy: 0.9832\n"
          ]
        }
      ],
      "source": [
        "print(\"Clean accuracy:\", accuracy(x_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOyLD0qn15X7"
      },
      "source": [
        "## Red Team Agent (Attack Planner)\n",
        "\n",
        "### Responsibility\n",
        "\n",
        "- Select attack type\n",
        "- Escalate attack strength\n",
        "- Stop when risk threshold breached\n",
        "\n",
        "### Adversarial Attacks\n",
        "\n",
        "- FGSM: Fast, single-step attack\n",
        "- PGD: Strong, iterative attack (industry standard)\n",
        "- These simulate malicious input manipulation.\n",
        "\n",
        "### Example Strategy\n",
        "\n",
        "- Start with FGSM\n",
        "- Increase Œµ until accuracy < threshold\n",
        "- Escalate to PGD\n",
        "- Record worst-case robustness\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7o0EtNr5vkIx"
      },
      "outputs": [],
      "source": [
        "class RedTeamAgent:\n",
        "    def __init__(self, classifier, accuracy_fn):\n",
        "        self.classifier = classifier\n",
        "        self.accuracy_fn = accuracy_fn\n",
        "\n",
        "    def run(self, x, y):\n",
        "        x = ensure_float32(x)\n",
        "        results = []\n",
        "\n",
        "        for eps in [0.05, 0.1, 0.2, 0.3]:\n",
        "            fgsm = FastGradientMethod(self.classifier, eps=eps)\n",
        "            x_adv = ensure_float32(fgsm.generate(x))\n",
        "            results.append({\n",
        "                \"attack\": \"FGSM\",\n",
        "                \"epsilon\": eps,\n",
        "                \"accuracy\": self.accuracy_fn(x_adv, y)\n",
        "            })\n",
        "\n",
        "        pgd = ProjectedGradientDescent(\n",
        "            self.classifier, eps=0.3, eps_step=0.05, max_iter=40\n",
        "        )\n",
        "        x_adv = ensure_float32(pgd.generate(x))\n",
        "        results.append({\n",
        "            \"attack\": \"PGD\",\n",
        "            \"epsilon\": 0.3,\n",
        "            \"accuracy\": self.accuracy_fn(x_adv, y)\n",
        "        })\n",
        "\n",
        "        return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7ppJpsS2nt6"
      },
      "source": [
        "## Evaluator Agent (Risk Quantification)\n",
        "\n",
        "### Responsibility\n",
        "\n",
        "- Turn failures into risk metrics\n",
        "- Produce audit-ready outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "NxvAKUNZ2oDu"
      },
      "outputs": [],
      "source": [
        "class EvaluatorAgent:\n",
        "    def summarize(self, attack_results):\n",
        "        worst = min(attack_results, key=lambda r: r[\"accuracy\"])\n",
        "        return {\n",
        "            \"worst_attack\": worst[\"attack\"],\n",
        "            \"worst_accuracy\": worst[\"accuracy\"]\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HduJdBKJ2zgQ"
      },
      "source": [
        "# Blue Team Agent (Defense & Risk Treatment)\n",
        "\n",
        "## Responsibility\n",
        "\n",
        "- Apply defenses\n",
        "- Re-test robustness\n",
        "- Decide if residual risk is acceptable\n",
        "\n",
        "## Defense: Feature Squeezing\n",
        "### Feature squeezing:\n",
        "\n",
        "- Reduces input precision\n",
        "- Removes adversarial noise\n",
        "- Is fast and low-cost\n",
        "- This is a defensive control, not a silver bullet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "JhPOAJVB56mR"
      },
      "outputs": [],
      "source": [
        "class BlueTeamAgent:\n",
        "    def __init__(self, classifier, accuracy_fn):\n",
        "        self.classifier = classifier\n",
        "        self.accuracy_fn = accuracy_fn\n",
        "        self.squeezer = FeatureSqueezing(bit_depth=5, clip_values=(0.0, 1.0))\n",
        "        self.fgsm = FastGradientMethod(estimator=self.classifier, eps=0.2)\n",
        "        self.pgd = ProjectedGradientDescent(\n",
        "            estimator=self.classifier,\n",
        "            eps=0.3,\n",
        "            eps_step=0.05,\n",
        "            max_iter=40\n",
        "        )\n",
        "\n",
        "    def defend_and_test(self, x, y):\n",
        "        x = ensure_float32(x)\n",
        "\n",
        "        # Clean ‚Üí Defense\n",
        "        x_clean_def = ensure_float32(self.squeezer(x)[0])\n",
        "        clean_acc = self.accuracy_fn(x_clean_def, y)\n",
        "\n",
        "        # FGSM ‚Üí Defense\n",
        "        x_fgsm = ensure_float32(self.fgsm.generate(x))\n",
        "        x_fgsm_def = ensure_float32(self.squeezer(x_fgsm)[0])\n",
        "        fgsm_acc = self.accuracy_fn(x_fgsm_def, y)\n",
        "\n",
        "        # PGD ‚Üí Defense\n",
        "        x_pgd = ensure_float32(self.pgd.generate(x))\n",
        "        x_pgd_def = ensure_float32(self.squeezer(x_pgd)[0])\n",
        "        pgd_acc = self.accuracy_fn(x_pgd_def, y)\n",
        "\n",
        "        return {\n",
        "            \"defense\": \"FeatureSqueezing\",\n",
        "            \"clean_defended_accuracy\": clean_acc,\n",
        "            \"fgsm_defended_accuracy\": fgsm_acc,\n",
        "            \"pgd_defended_accuracy\": pgd_acc\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gPeno2mUw7u"
      },
      "source": [
        "## Detector Agent (Detection-Based Control)\n",
        "\n",
        "The Detector Agent implements a **detection-based compensating control** for adversarial and abusive inputs.\n",
        "\n",
        "### Why this agent exists\n",
        "Even with adversarial training, no model can be made fully robust against all adaptive attacks.  \n",
        "Instead of relying solely on prevention, the Detector Agent enables **early identification and containment** of suspicious inputs.\n",
        "\n",
        "This aligns with real-world security practice:\n",
        "- *Prevent what you can*\n",
        "- *Detect what you can‚Äôt fully prevent*\n",
        "- *Contain and escalate residual risk*\n",
        "\n",
        "### What the Detector Agent does\n",
        "The Detector Agent:\n",
        "- Learns to distinguish **clean inputs vs adversarial inputs**\n",
        "- Flags inputs that exhibit adversarial characteristics\n",
        "- Produces a **detector flag rate**, which is used as a governance signal\n",
        "\n",
        "### Detector flag rate (key metric)\n",
        "The **detector flag rate** is defined as:\n",
        "\n",
        "> The fraction of inputs flagged as adversarial or anomalous in a given batch or time window.\n",
        "\n",
        "This metric answers:\n",
        "- *‚ÄúHow hostile is the input environment?‚Äù*\n",
        "- *‚ÄúAre we under active attack or abuse?‚Äù*\n",
        "\n",
        "### How it is used in governance\n",
        "The detector flag rate is combined with robustness metrics (PGD / FGSM accuracy) in the **Multi-Metric Risk Gate** to:\n",
        "- Trigger escalation\n",
        "- Enable human-in-the-loop routing\n",
        "- Activate compensating controls\n",
        "\n",
        "### Why detection is critical\n",
        "Detection provides:\n",
        "- Protection against unknown or future attacks\n",
        "- Defense against abuse even when accuracy remains high\n",
        "- Evidence of active threat conditions\n",
        "\n",
        "### Governance alignment\n",
        "- **ISO/IEC 42001:** Operational monitoring and compensating controls  \n",
        "- **NIST AI RMF:** MANAGE function (risk containment)  \n",
        "- **OWASP AI:** Input Manipulation, Model Abuse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "pzQM9vlcUxSA"
      },
      "outputs": [],
      "source": [
        "class DetectorAgent:\n",
        "    def __init__(self):\n",
        "        self.detector = None\n",
        "\n",
        "    def train(self, x_clean, x_adv):\n",
        "        x = np.concatenate([x_clean, x_adv]).reshape(len(x_clean)*2, -1)\n",
        "        y = np.concatenate([np.zeros(len(x_clean)), np.ones(len(x_adv))])\n",
        "\n",
        "        model = LogisticRegression(max_iter=1000)\n",
        "        model.fit(x, y)\n",
        "\n",
        "        self.detector = SklearnClassifier(model=model, clip_values=(0,1))\n",
        "\n",
        "    def flag_rate(self, x):\n",
        "        x = x.reshape(len(x), -1)\n",
        "        preds = self.detector.predict(x)\n",
        "        return np.argmax(preds, axis=1).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msNOAfqxJiTN"
      },
      "source": [
        "# Adversarial Training Agent\n",
        "\n",
        "## This agent is only triggered on escalation.\n",
        "\n",
        "### What it does (simply)\n",
        "\n",
        "- Generates PGD adversarial examples\n",
        "- Mixes them with clean data\n",
        "- Retrains the model\n",
        "- Updates the ART classifier\n",
        "\n",
        "### Why this design?\n",
        "\n",
        "- Explicit\n",
        "- Deterministic\n",
        "- No hidden callbacks\n",
        "- Easy to audit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "qSumzRCs3Cis"
      },
      "outputs": [],
      "source": [
        "class AdversarialTrainingAgent:\n",
        "    def __init__(self, classifier, model, optimizer, loss_fn):\n",
        "        self.classifier = classifier\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "\n",
        "        self.pgd = ProjectedGradientDescent(\n",
        "            estimator=self.classifier,\n",
        "            eps=0.3,\n",
        "            eps_step=0.05,\n",
        "            max_iter=40\n",
        "        )\n",
        "\n",
        "    def retrain(self, x_train, y_train, epochs=2, adv_ratio=0.5):\n",
        "        x_train = ensure_float32(x_train)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Generate PGD adversarial examples\n",
        "            x_adv = ensure_float32(self.pgd.generate(x_train))\n",
        "\n",
        "            split = int(len(x_train) * adv_ratio)\n",
        "            x_mix = np.concatenate([x_train[:split], x_adv[:split]])\n",
        "            y_mix = np.concatenate([y_train[:split], y_train[:split]])\n",
        "\n",
        "            x_mix_t = torch.from_numpy(x_mix)\n",
        "            y_mix_t = torch.from_numpy(y_mix)\n",
        "\n",
        "            self.model.train()\n",
        "            self.optimizer.zero_grad()\n",
        "            loss = self.loss_fn(self.model(x_mix_t), y_mix_t)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            print(f\"[ADV TRAIN] Epoch {epoch+1} | Loss={loss.item():.4f}\")\n",
        "\n",
        "        return {\n",
        "            \"status\": \"RETRAINED\",\n",
        "            \"epochs\": epochs,\n",
        "            \"adv_ratio\": adv_ratio\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgi4BvUxjwjP"
      },
      "source": [
        "## MultiMetricRiskGate\n",
        "The MultiMetricRiskGate evaluates **multiple risk signals together** to make a single governance decision.\n",
        "\n",
        "Signals evaluated:\n",
        "- PGD defended accuracy (worst-case robustness)\n",
        "- FGSM defended accuracy (casual attacker)\n",
        "- Detector flag rate (active abuse signal)\n",
        "- Clean accuracy (business impact)\n",
        "\n",
        "This avoids single-metric blind spots and enables policy-driven, enterprise-grade risk decisions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "4YASbRCPV6Y0"
      },
      "outputs": [],
      "source": [
        "class MultiMetricRiskGate:\n",
        "    def __init__(self, policy):\n",
        "        self.metrics = policy[\"metrics\"]\n",
        "        self.actions = policy[\"actions\"]\n",
        "\n",
        "    def evaluate(self, observed):\n",
        "        triggered = []\n",
        "        score = 0.0\n",
        "\n",
        "        for name, rule in self.metrics.items():\n",
        "            val = observed[name]\n",
        "            w = rule[\"weight\"]\n",
        "\n",
        "            if \"escalate_below\" in rule and val < rule[\"escalate_below\"]:\n",
        "                triggered.append(name); score += w\n",
        "            elif \"warn_below\" in rule and val < rule[\"warn_below\"]:\n",
        "                score += 0.5 * w\n",
        "\n",
        "            if \"escalate_above\" in rule and val > rule[\"escalate_above\"]:\n",
        "                triggered.append(name); score += w\n",
        "\n",
        "        if triggered:\n",
        "            return {**self.actions[\"escalate\"], \"triggered_by\": triggered, \"risk_score\": score}\n",
        "\n",
        "        return {**self.actions[\"accept\"], \"risk_score\": score}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJvi-XrbjwjP"
      },
      "source": [
        "## RiskPolicyLoader\n",
        "This component loads the **external risk policy** from YAML.\n",
        "\n",
        "Why this matters:\n",
        "- Governance rules are **decoupled from code**\n",
        "- Risk thresholds can be changed without redeployment\n",
        "- Policies are version-controlled and auditable\n",
        "\n",
        "This aligns with ISO/NIST expectations for documented, reviewable risk criteria.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "gaoHYR4QWM2n"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "\n",
        "class RiskPolicyLoader:\n",
        "    @staticmethod\n",
        "    def load(path: str) -> dict:\n",
        "        with open(path, \"r\") as f:\n",
        "            return yaml.safe_load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3tU1-OR29P7"
      },
      "source": [
        "# Orchestrating the Red Team Exercise\n",
        "\n",
        "## Engineering reasons\n",
        "\n",
        "- Owns state (model lifecycle)\n",
        "- Owns control flow\n",
        "- Coordinates agents\n",
        "- Enforces contracts\n",
        "- Governance reasons\n",
        "- Single point of decision authority\n",
        "- Explicit risk acceptance logic\n",
        "- Auditable ‚Äúwho decided what and why‚Äù\n",
        "\n",
        "### This aligns directly with:\n",
        "\n",
        "- ISO/IEC 42001 (decision ownership)\n",
        "- NIST AI RMF (GOVERN + MANAGE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "xeNyQ4fW29bf"
      },
      "outputs": [],
      "source": [
        "class AIOrchestrator:\n",
        "    \"\"\"\n",
        "    Central governance and execution authority.\n",
        "\n",
        "    Responsibilities:\n",
        "    - Run red-team attacks\n",
        "    - Evaluate worst-case robustness\n",
        "    - Apply blue-team defenses\n",
        "    - Measure detection signals\n",
        "    - Aggregate metrics\n",
        "    - Enforce policy-driven risk decisions\n",
        "    - Trigger adversarial training on escalation\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        classifier,\n",
        "        model,\n",
        "        optimizer,\n",
        "        loss_fn,\n",
        "        accuracy_fn,\n",
        "        policy_path\n",
        "    ):\n",
        "        # --- Agents ---\n",
        "        self.red_agent = RedTeamAgent(classifier, accuracy_fn)\n",
        "        self.eval_agent = EvaluatorAgent()\n",
        "        self.blue_agent = BlueTeamAgent(classifier, accuracy_fn)\n",
        "        self.detector_agent = DetectorAgent()\n",
        "        self.adv_train_agent = AdversarialTrainingAgent(\n",
        "            classifier, model, optimizer, loss_fn\n",
        "        )\n",
        "\n",
        "        # --- Governance ---\n",
        "        policy = RiskPolicyLoader.load(policy_path)\n",
        "        self.risk_gate = MultiMetricRiskGate(policy)\n",
        "\n",
        "        self.classifier = classifier\n",
        "\n",
        "    def run(\n",
        "        self,\n",
        "        x_test,\n",
        "        y_test,\n",
        "        retrain_on_escalation=True,\n",
        "        retrain_epochs=2,\n",
        "        retrain_adv_ratio=0.5,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Executes one full governance cycle.\n",
        "        \"\"\"\n",
        "\n",
        "        # ----------------------------\n",
        "        # 1. Red Team (Attacks)\n",
        "        # ----------------------------\n",
        "        attack_results = self.red_agent.run(x_test, y_test)\n",
        "\n",
        "        # ----------------------------\n",
        "        # 2. Evaluation (Worst Case)\n",
        "        # ----------------------------\n",
        "        evaluation_summary = self.eval_agent.summarize(attack_results)\n",
        "\n",
        "        # ----------------------------\n",
        "        # 3. Blue Team (Defense + Attacks)\n",
        "        # ----------------------------\n",
        "        defense_results = self.blue_agent.defend_and_test(x_test, y_test)\n",
        "\n",
        "        # ----------------------------\n",
        "        # 4. Detection Signal\n",
        "        # ----------------------------\n",
        "        pgd = ProjectedGradientDescent(\n",
        "            estimator=self.classifier,\n",
        "            eps=0.3,\n",
        "            eps_step=0.05,\n",
        "            max_iter=40\n",
        "        )\n",
        "\n",
        "        x_adv = ensure_float32(pgd.generate(x_test))\n",
        "\n",
        "        # Train detector on a subset (governance-friendly, bounded)\n",
        "        self.detector_agent.train(\n",
        "            x_clean=x_test[:2000],\n",
        "            x_adv=x_adv[:2000]\n",
        "        )\n",
        "\n",
        "        detector_flag_rate = self.detector_agent.flag_rate(x_test)\n",
        "\n",
        "        # ----------------------------\n",
        "        # 5. Aggregate Metrics\n",
        "        # ----------------------------\n",
        "        observed_metrics = {\n",
        "            \"pgd_defended_accuracy\": defense_results[\"pgd_defended_accuracy\"],\n",
        "            \"fgsm_defended_accuracy\": defense_results[\"fgsm_defended_accuracy\"],\n",
        "            \"clean_accuracy\": defense_results[\"clean_defended_accuracy\"],\n",
        "            \"detector_flag_rate\": detector_flag_rate,\n",
        "        }\n",
        "\n",
        "        # ----------------------------\n",
        "        # 6. Policy-Driven Risk Decision\n",
        "        # ----------------------------\n",
        "        decision = self.risk_gate.evaluate(observed_metrics)\n",
        "\n",
        "        result = {\n",
        "            \"attack_results\": attack_results,\n",
        "            \"evaluation\": evaluation_summary,\n",
        "            \"defense\": defense_results,\n",
        "            \"metrics\": observed_metrics,\n",
        "            \"decision\": decision,\n",
        "        }\n",
        "\n",
        "        # ----------------------------\n",
        "        # 7. Automatic Mitigation (if allowed)\n",
        "        # ----------------------------\n",
        "        if (\n",
        "            decision[\"decision\"] == \"ESCALATE\"\n",
        "            and retrain_on_escalation\n",
        "        ):\n",
        "            print(\"üö® Risk escalation triggered ‚Üí adversarial training\")\n",
        "\n",
        "            retrain_info = self.adv_train_agent.retrain(\n",
        "                x_train=x_test[:5000],\n",
        "                y_train=y_test[:5000],\n",
        "                epochs=retrain_epochs,\n",
        "                adv_ratio=retrain_adv_ratio,\n",
        "            )\n",
        "\n",
        "            # ----------------------------\n",
        "            # 8. Post-Retraining Re-evaluation\n",
        "            # ----------------------------\n",
        "            print(\"üîÅ Re-running governance cycle after retraining\")\n",
        "\n",
        "            post_attack_results = self.red_agent.run(x_test, y_test)\n",
        "            post_eval_summary = self.eval_agent.summarize(post_attack_results)\n",
        "            post_defense_results = self.blue_agent.defend_and_test(x_test, y_test)\n",
        "\n",
        "            x_adv_post = ensure_float32(pgd.generate(x_test))\n",
        "            self.detector_agent.train(\n",
        "                x_clean=x_test[:2000],\n",
        "                x_adv=x_adv_post[:2000]\n",
        "            )\n",
        "            post_detector_flag_rate = self.detector_agent.flag_rate(x_test)\n",
        "\n",
        "            post_metrics = {\n",
        "                \"pgd_defended_accuracy\": post_defense_results[\"pgd_defended_accuracy\"],\n",
        "                \"fgsm_defended_accuracy\": post_defense_results[\"fgsm_defended_accuracy\"],\n",
        "                \"clean_accuracy\": post_defense_results[\"clean_defended_accuracy\"],\n",
        "                \"detector_flag_rate\": post_detector_flag_rate,\n",
        "            }\n",
        "\n",
        "            post_decision = self.risk_gate.evaluate(post_metrics)\n",
        "\n",
        "            result[\"post_training\"] = {\n",
        "                \"retraining\": retrain_info,\n",
        "                \"attack_results\": post_attack_results,\n",
        "                \"evaluation\": post_eval_summary,\n",
        "                \"defense\": post_defense_results,\n",
        "                \"metrics\": post_metrics,\n",
        "                \"decision\": post_decision,\n",
        "            }\n",
        "\n",
        "        return result"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Call Orchestrator to execute the attack"
      ],
      "metadata": {
        "id": "lW3tz7wflg-I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSw0vhOBM-C9"
      },
      "outputs": [],
      "source": [
        "orchestrator = AIOrchestrator(\n",
        "    classifier=classifier,\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    loss_fn=criterion,\n",
        "    accuracy_fn=accuracy,\n",
        "    policy_path=\"risk_policy.yaml\"\n",
        ")\n",
        "\n",
        "results = orchestrator.run(x_test, y_test)\n",
        "print(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zbh4pseRs5Zx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}